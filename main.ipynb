{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cdc9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "\n",
    "opts = webdriver.ChromeOptions()\n",
    "opts.add_experimental_option('prefs', {\n",
    "    'download.default_directory': r\"C:\\Users\\USER\\Downloads\"\n",
    "})\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\USER\\Downloads\\chromedriver.exe\", options=opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb00574",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://movie.daum.net/moviedb/grade?movieId=##MOVIENO##\"\"\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "comment_num_txt = soup.find('span', {'class': 'txt_netizen'}) #총 data 수\n",
    "comment_num = int(comment_num_txt.text[1:-2])\n",
    "\n",
    "# 클릭 수 = (총 data 수 - 10) / 30(클릭 당 새로 호출되는 data 수)\n",
    "click_num = (comment_num - 10) / 30\n",
    "click_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62586614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 최신 순으로 조회\n",
    "new_button = '//*[@class=\"link_cate #latest\"]'\n",
    "driver.find_element('xpath', new_button).click()\n",
    "\n",
    "# fetch more\n",
    "for i in tqdm(range(math.ceil(click_num))):\n",
    "    more_button = '//*[@class=\"link_fold #more\"]'\n",
    "    driver.find_element('xpath', more_button).click()\n",
    "    time.sleep(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89bb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 가져오기\n",
    "html = driver.page_source  #현재 페이지 정보 갱신\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "tmp_list = []   # data 저장 리스트\n",
    "review = soup.find_all('p', {'class': 'desc_txt font_size_'})\n",
    "\n",
    "for p in review:\n",
    "    ptext = p.text\n",
    "    a = ptext.replace('\\n', '') \n",
    "    tmp_list.append(a.strip())\n",
    "\n",
    "# 앞 3 데이터 check  \n",
    "tmp_list[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f362e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b576e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_data 가져오기(data 있다면)\n",
    "\n",
    "comments = soup.find(\"ul\", {\"class\": \"list_comment\"}).find_all(\"li\")\n",
    "star_list = []\n",
    "\n",
    "for abc in comments:\n",
    "    if abc.find('p', {'class': 'desc_txt font_size_'}):\n",
    "        ans = abc.find_all(\"div\")[1]\n",
    "        star_list.append(ans.text.strip())\n",
    "        \n",
    "star_list[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38646b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(star_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ddd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# star_list에서의 pos/neg data 인덱스\n",
    "pos_index = []\n",
    "neg_index = []\n",
    "\n",
    "for i in range(len(star_list)):\n",
    "    if int(star_list[i]) < 8:\n",
    "        neg_index.append(i)\n",
    "    else:\n",
    "        pos_index.append(i)\n",
    "        \n",
    "# pos/neg data 리스트 생성\n",
    "pos_list = []\n",
    "neg_list = []\n",
    "\n",
    "for i in range(len(pos_index)):\n",
    "    pos_list.append(tmp_list[pos_index[i]])\n",
    "    \n",
    "for i in range(len(neg_index)):\n",
    "    neg_list.append(tmp_list[neg_index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze\n",
    "from konlpy.tag import Okt  # pip install konlpy\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "okt = Okt()\n",
    "\n",
    "# 전체 data 분석\n",
    "all_doc = '\\n'.join(tmp_list)\n",
    "all_doc_noun = okt.nouns(all_doc)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "okt = Okt()\n",
    "\n",
    "# pos data 분석\n",
    "pos_doc = '\\n'.join(pos_list)\n",
    "pos_doc_noun = okt.nouns(pos_doc)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b927b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "okt = Okt()\n",
    "\n",
    "# neg data 분석\n",
    "neg_doc = '\\n'.join(neg_list)\n",
    "neg_doc_noun = okt.nouns(neg_doc)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud  # pip install wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "count_noun = Counter(pos_doc_noun)\n",
    "\n",
    "stopword_list = ['DEFAULT']\n",
    "\n",
    "for word in count_noun:\n",
    "    if len(word) == 1:\n",
    "        stopword_list.append(word)\n",
    "        \n",
    "stopword_list[-5:]\n",
    "\n",
    "for stopword in stopword_list:\n",
    "    if stopword in count_noun:\n",
    "        count_noun.pop(stopword)\n",
    "\n",
    "wc_noun = WordCloud(background_color='white', max_words=2000, font_path=r'C:\\Windows\\Fonts\\H2MJRE.ttf')\n",
    "wc_noun = wc_noun.generate_from_frequencies(count_noun)\n",
    "\n",
    "plt.imshow(wc_noun, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_noun = Counter(neg_doc_noun)\n",
    "\n",
    "stopword_list = ['DEFAULT']\n",
    "\n",
    "for word in count_noun:\n",
    "    if len(word) == 1:\n",
    "        stopword_list.append(word)\n",
    "        \n",
    "stopword_list[-5:]\n",
    "\n",
    "for stopword in stopword_list:\n",
    "    if stopword in count_noun:\n",
    "        count_noun.pop(stopword)\n",
    "\n",
    "wc_noun = WordCloud(background_color='white', max_words=2000, font_path=r'C:\\Windows\\Fonts\\H2MJRE.ttf')\n",
    "wc_noun = wc_noun.generate_from_frequencies(count_noun)\n",
    "\n",
    "plt.imshow(wc_noun, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8358d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "star_posneg = []\n",
    "\n",
    "for i in range(len(star_list)):\n",
    "    if int(star_list[i]) < 8:\n",
    "        star_posneg.append(0)\n",
    "    else:\n",
    "        star_posneg.append(1) \n",
    "\n",
    "# dataframe 생성\n",
    "df = pd.DataFrame({'id': 0, 'data': tmp_list, 'ratings': star_posneg})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341078bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt파일 생성\n",
    "df.to_csv('data1.txt', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410eb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import urllib.request \n",
    "\n",
    "def load_data(file_path):\n",
    "    train = []\n",
    "    count = 0\n",
    "    with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            if count == 3000: break\n",
    "            line = line.strip()\n",
    "            id, doc, label = line.split('\\t')\n",
    "            if label == '1': label = 'pos'\n",
    "            elif label == '0': label = 'neg'\n",
    "            train.append((doc, label))\n",
    "            count += 1\n",
    "            \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb95ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_data('data1.txt')\n",
    "train = train[1:]\n",
    "print(train[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd4bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tokenize(raw_sent):\n",
    "    pos_sent =[]\n",
    "    \n",
    "    sent = okt.pos(raw_sent, norm=True, stem=True)\n",
    "    \n",
    "    for tup in sent:\n",
    "        word, tag = tup[0], tup[1]\n",
    "        word_tag = word + '/' + tag\n",
    "        pos_sent.append(word_tag)\n",
    "        \n",
    "    return ' '.join(pos_sent)\n",
    "\n",
    "def make_word_dict(train, use_morph=False):\n",
    "    all_words = set()\n",
    "\n",
    "    for tup in train:\n",
    "        sent, label = tup[0], tup[1]\n",
    "        if use_morph: sent = pos_tokenize(sent)\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            all_words.add(word)\n",
    "            \n",
    "    return all_words\n",
    "\n",
    "\n",
    "def make_train_feats(train, all_words, use_morph=False):\n",
    "    train_features = []\n",
    "\n",
    "    for tup in train:\n",
    "        sent, label = tup[0], tup[1]\n",
    "        if use_morph: sent = pos_tokenize(sent)\n",
    "        words = word_tokenize(sent)\n",
    "        tmp = {set_word: (set_word in words) for set_word in all_words}\n",
    "        sent_tup = (tmp, label)\n",
    "        train_features.append(sent_tup)\n",
    "        \n",
    "    return train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b10291",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_morph = True\n",
    "all_words = make_word_dict(train, use_morph)\n",
    "print('word 집합 개수: ', len(all_words))\n",
    "train_features = make_train_feats(train, all_words, use_morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f189e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3441945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNaiveBayesClassifier:\n",
    "    def __init__(self, k=0.5, use_morph=False):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "        self.use_morph = use_morph\n",
    "        \n",
    "        if self.use_morph:\n",
    "            from konlpy.tag import Okt\n",
    "            self.okt = Okt()\n",
    "            \n",
    "    def load_data(self, file_path):\n",
    "        docs = []\n",
    "        labels = []\n",
    "        \n",
    "        count = 0\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                if count == 500:\n",
    "                    break\n",
    "                line = line.strip()\n",
    "                id, doc, label = line.split('\\t')\n",
    "                docs.append(doc)\n",
    "                if label == '1': label = 'pos'\n",
    "                elif label == '0': label = 'neg'\n",
    "                labels.append(label)\n",
    "                count += 1\n",
    "            \n",
    "        return docs[1:], labels[1:]\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        if self.use_morph:\n",
    "            pos_sent = []\n",
    "            \n",
    "            sent = self.okt.pos(sentence, norm=True, stem=True)\n",
    "            \n",
    "            for tup in sent:\n",
    "                word, tag = tup\n",
    "                word_tag = word + '/' + tag\n",
    "                pos_sent.append(word_tag)\n",
    "                \n",
    "            sentence = ' '.join(pos_sent)\n",
    "            \n",
    "        return sentence.split()\n",
    "    \n",
    "    def count_words(self, docs, labels):\n",
    "        count_dict = dict()\n",
    "        for doc, label in zip(docs, labels):\n",
    "            for word in self.tokenize(doc):\n",
    "                if word not in count_dict:\n",
    "                    count_dict[word] = {'pos': 0, 'neg':0}\n",
    "                count_dict[word][label] += 1\n",
    "        \n",
    "        print('num of words...', len(count_dict))\n",
    "        return count_dict\n",
    "    \n",
    "    def word_prob(self, count_dict, pos_class_num, neg_class_num, k):\n",
    "        word_prob_list = []\n",
    "        \n",
    "        for word in count_dict:\n",
    "            pos_word_num = count_dict[word]['pos']\n",
    "            neg_word_num = count_dict[word]['neg']\n",
    "            \n",
    "            pos_class_prob = (pos_word_num + k) / (pos_class_num + 2*k)\n",
    "            neg_class_prob = (neg_word_num + k) / (neg_class_num + 2*k)\n",
    "            \n",
    "            tup = (word, pos_class_prob, neg_class_prob)\n",
    "            word_prob_list.append(tup)\n",
    "        \n",
    "        self.word_prob_list = word_prob_list\n",
    "        return word_prob_list\n",
    "    \n",
    "    def class_prob(self, word_prob_list, test_sentence, use_unseen=False):\n",
    "        test_words = self.tokenize(test_sentence)\n",
    "        \n",
    "        sent_log_pos_class_prob, sent_log_neg_class_prob = 0.0, 0.0\n",
    "        \n",
    "        for word, word_pos_class_prob, word_neg_class_prob in word_prob_list:\n",
    "            if word in test_words:\n",
    "                sent_log_pos_class_prob += math.log(word_pos_class_prob) \n",
    "                sent_log_neg_class_prob += math.log(word_neg_class_prob)\n",
    "            else:\n",
    "                if use_unseen:\n",
    "                    sent_log_pos_class_prob += math.log(1-word_pos_class_prob) \n",
    "                    sent_log_neg_class_prob += math.log(1-word_neg_class_prob)\n",
    "            \n",
    "        sent_pos_class_prob = math.exp(sent_log_pos_class_prob)\n",
    "        sent_neg_class_prob = math.exp(sent_log_neg_class_prob)\n",
    "        \n",
    "        pos_class_prob = sent_pos_class_prob/(sent_pos_class_prob+sent_neg_class_prob)\n",
    "        neg_class_prob = sent_neg_class_prob/(sent_pos_class_prob+sent_neg_class_prob)\n",
    "        \n",
    "        return pos_class_prob, neg_class_prob\n",
    "    \n",
    "    def train(self, train_file_path):\n",
    "        train_docs, train_labels = self.load_data(train_file_path)\n",
    "        \n",
    "        word_count_dict = self.count_words(train_docs, train_labels)\n",
    "        \n",
    "        pos_class_num = len([label for label in train_labels if label == 'pos'])\n",
    "        neg_class_num = len([label for label in train_labels if label == 'neg'])\n",
    "        \n",
    "        self.word_probs = self.word_prob(word_count_dict, pos_class_num, neg_class_num, self.k)\n",
    "\n",
    "    def classify(self, test_sentence, use_unseen=False):\n",
    "        pos_class_prob, neg_class_prob = self.class_prob(self.word_prob_list, test_sentence, use_unseen)\n",
    "        \n",
    "        if pos_class_prob > neg_class_prob:\n",
    "            print('pos', pos_class_prob)\n",
    "        else:\n",
    "            print('neg', neg_class_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d1a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MyNaiveBayesClassifier(use_morph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f11b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train('data1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665bbbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.classify('SAMPLE TXT GOES HERE', use_unseen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedb0bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "\n",
    "star_int_list = []\n",
    "tempo = 0\n",
    "\n",
    "for i in range(len(star_list)):\n",
    "    tempo += int(star_list[i])\n",
    "    star_int_list.append(tempo/ (i+1))\n",
    "    \n",
    "reverse_list = star_int_list[::-1]\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.title('data 변동성')\n",
    "plt.plot(reverse_list, color='crimson', label='data1')\n",
    "plt.xlabel('data 수')\n",
    "plt.ylabel('ratings AVG')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
